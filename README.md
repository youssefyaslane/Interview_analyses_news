# FT Daily Pipeline â€” Config, DB & Scraper (Mongo + SeleniumBase)

Ce projet met en place un pipeline dâ€™analyse dâ€™articles FT (liens, contenus, analyses LLM). 

---

## ðŸ“‚ Arborescence (extrait)
```
Prject_analyse_news/
â”œâ”€ app/
â”‚  â”œâ”€ __init__.py
â”‚  â”œâ”€ config.py 
â”‚  â”œâ”€ main.py                  # Pipeline dâ€™analyse dâ€™articles
â”‚  â”œâ”€ Database/
â”‚  â”‚   â”œâ”€ __init__.py
â”‚  â”‚   â””â”€ db.py                 # connexion Mongo, index, upsert_link / upsert_article / upsert_analysis
â”‚  â”œâ”€ scraping/
â”‚  â”‚   â”œâ”€ scrape_links.py       # scraper SeleniumBase â†’ Mongo (ft_links)
â”‚  â”‚   â”œâ”€ fetch_articles.py  # scraper SeleniumBase â†’ Mongo (ft_articles)
â”‚  â”‚   â””â”€ utils.py              # utilitaires scraping (scroll, cookies, clean text)
â”‚  â”œâ”€ analysis/
â”‚  â”‚   â””â”€ analyze_with_Gemini.py   # Ã‰tape 4: LangChain + Gemini â†’ ft_analyses
â”‚  â””â”€ test/
â”‚     â”œâ”€ __init__.py
â”‚     â””â”€ test_db.py             # smoke test dâ€™insertion/lecture
â”œâ”€ requirements.txt
â”œâ”€ .env.example                 # gabarit des variables dâ€™env
â”œâ”€ .gitignore
â””â”€ README.md
```

---

## âš™ï¸ PrÃ©requis
- Python 3.10+
- MongoDB local ou Atlas (URI)
- Chrome/Chromium installÃ© (SeleniumBase sâ€™en sert)
- (optionnel) Profil Chrome avec extension chargÃ©e (voir README prÃ©cÃ©dent pour dÃ©tails)

---

## ðŸš€ Installation rapide 
```
python -m venv venv
venv\Scripts\activate
pip install -r requirements.txt
copy .env.example .env
```

Ajoute ensuite ce contenu dans **.env** :
```env
MONGODB_URI=mongodb://localhost:27017
MONGODB_DB=ft_daily
MONGODB_LINKS_COLLECTION=ft_links
MONGODB_ARTICLES_COLLECTION=ft_articles
MONGODB_ANALYSES_COLLECTION=ft_analyses

SCRAPER_USER_AGENT=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36

# LLM (Ã‰tape 4)
GOOGLE_API_KEY=YOUR_API_KEY
LLM_MODEL=gemini-1.5-flash
```

---

## â–¶ï¸ Lancer les tests & scrapers

### 1. Tester la DB
```bash
python -m app.test.test_db
```

### 2. Scraper les liens publics FT â†’ `ft_links`
```bash
python -m app.scraping.scrape_links
```
Exemple de sortie :
```
=== https://www.ft.com/ ===
[INFO] https://www.ft.com/ -> 30 liens trouvÃ©s
[OK] 30 liens upsertÃ©s pour https://www.ft.com/
...
âœ… TerminÃ©. Total liens upsertÃ©s: 121
```

### 3. Scraper les articles complets FT â†’ `ft_articles`
```bash
python -m app.scraping.fetch_articles
```
Exemple de document insÃ©rÃ© dans **ft_articles** :
```json
{
  "url": "https://www.ft.com/content/xxxx",
  "title": "Titre complet",
  "body_article": "Tout le texte intÃ©gral de lâ€™article",
  "word_count": 1420,
  "fetched_at": "2025-08-27T12:30:00Z"
}
```

---

## ðŸ”Ž Ã‰tape 4 â€” Analyse LLM (LangChain + Gemini) â†’ `ft_analyses`

### Lancer lâ€™analyse
```bash
python -m app.analysis.analyze_with_langchain
```
- SÃ©lectionne les articles **non encore analysÃ©s** (body_article non vide)
- Envoie Ã  Gemini via **LangChain**
- Sauvegarde dans `ft_analyses` (**sans** `risks_or_implications`)

**Exemple de document `ft_analyses` :**
```json
{
  "url": "https://www.ft.com/content/xxxx",
  "model": "gemini-1.5-flash",
  "title_src": "Titre de l'article",
  "summary": "RÃ©sumÃ© concis ...",
  "top_topics": ["semiconductors", "US policy", "Intel"],
  "sentiment": "neutral",
  "entities": [{"type":"ORG","name":"Intel"}],
  "analyzed_at": "2025-08-27T13:10:00Z"
}
```
---
## ðŸ“Š Utilisation dans Power BI / Excel

- Power BI Desktop â†’ **Obtenir des donnÃ©es** â†’ Excel â†’ `data/articles_entities_dashboard.xlsx`
- Visuels conseillÃ©s :
  - Cartes KPI : *Total Articles*, *Avg Sentiment*, *Total Words*
  - Courbe : articles par jour (`ByDay`)
  - Barres : top entitÃ©s (`Entities`) / par section (`BySection`)
  - Anneau : rÃ©partition par `sentiment_label`
  - Treemap ou nuage de mots : `Topics`
  - Graphe rÃ©seau (visuel custom) : `Cooccurrence`

### ðŸ“¸ Dashboar
<img src="assets/img/dashbord_img.png" alt="FT Dashboard" width="1000"/>

---
## ðŸ“Œ Roadmap
1. âœ… Step 1 : Config + DB helpers  
2. âœ… Step 2 : Scraper liens publics â†’ `ft_links`  
3. âœ… Step 3 : Fetch articles complets â†’ `ft_articles`  
4. âœ… Step 4 : Analyse LLM (LangChain + Gemini) â†’ `ft_analyses`  
5. ðŸ”œ Step 5 : Orchestration Airflow (1 exÃ©cution/jour)  
6. âœ… Step 6 : Dashboard (Metabase / PowerBI)

---

## âœ¨ Auteur
**Youssef Yaslane** â€” *Data Scientist | Big Data & IA Engineer*
